{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bask/homes/c/cxx075/Chenguang/projects/nlp-notebooks/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/bask/homes/c/cxx075/Chenguang/projects/nlp-notebooks/.venv/lib/python3.12/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    XLNetForSequenceClassification,\n",
    "    XLNetTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Load AG News Dataset\n",
    "train_datapip = AG_NEWS(split=\"train\")  # type: ignore\n",
    "test_datapip = AG_NEWS(split=\"test\")  # type: ignore\n",
    "\n",
    "# Define tokenizer and model\n",
    "tokenizer: XLNetTokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "model = XLNetForSequenceClassification.from_pretrained(\n",
    "    \"xlnet-base-cased\", num_labels=4\n",
    ").to(DEVICE)\n",
    "tokenizer.__call__\n",
    "\n",
    "\n",
    "# Preprocessing and Tokenization function\n",
    "def preprocess(batch):\n",
    "    labels, texts = zip(*batch)\n",
    "    inputs = tokenizer(\n",
    "        list(texts), padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    "    )\n",
    "    labels = torch.tensor(labels) - 1  # Label 0-indexed for PyTorch\n",
    "    return inputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_datapip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9, Loss: 0.0870\n",
      "Step 19, Loss: 0.0803\n",
      "Step 29, Loss: 0.0764\n",
      "Step 39, Loss: 0.0662\n",
      "Step 49, Loss: 0.0711\n",
      "Step 59, Loss: 0.0685\n",
      "Step 69, Loss: 0.0718\n",
      "Step 79, Loss: 0.0686\n",
      "Step 89, Loss: 0.0667\n",
      "Step 99, Loss: 0.0659\n",
      "Step 109, Loss: 0.0703\n",
      "Step 119, Loss: 0.0697\n",
      "Step 129, Loss: 0.0695\n",
      "Step 139, Loss: 0.0736\n",
      "Step 149, Loss: 0.0730\n",
      "Step 159, Loss: 0.0751\n",
      "Step 169, Loss: 0.0758\n",
      "Step 179, Loss: 0.0782\n",
      "Step 189, Loss: 0.0782\n",
      "Step 199, Loss: 0.0805\n",
      "Step 209, Loss: 0.0809\n",
      "Step 219, Loss: 0.0812\n",
      "Step 229, Loss: 0.0838\n",
      "Step 239, Loss: 0.0847\n",
      "Step 249, Loss: 0.0852\n",
      "Step 259, Loss: 0.0859\n",
      "Step 269, Loss: 0.0858\n",
      "Step 279, Loss: 0.0883\n",
      "Step 289, Loss: 0.0877\n",
      "Step 299, Loss: 0.0872\n",
      "Step 309, Loss: 0.0891\n",
      "Step 319, Loss: 0.0893\n",
      "Step 329, Loss: 0.0887\n",
      "Step 339, Loss: 0.0898\n",
      "Step 349, Loss: 0.0897\n",
      "Step 359, Loss: 0.0889\n",
      "Step 369, Loss: 0.0901\n",
      "Step 379, Loss: 0.0905\n",
      "Step 389, Loss: 0.0923\n",
      "Step 399, Loss: 0.0933\n",
      "Step 409, Loss: 0.0938\n",
      "Step 419, Loss: 0.0932\n",
      "Step 429, Loss: 0.0944\n",
      "Step 439, Loss: 0.0950\n",
      "Step 449, Loss: 0.0962\n",
      "Step 459, Loss: 0.0966\n",
      "Step 469, Loss: 0.0966\n",
      "Step 479, Loss: 0.0966\n",
      "Step 489, Loss: 0.0962\n",
      "Step 499, Loss: 0.0976\n",
      "Step 509, Loss: 0.0981\n",
      "Step 519, Loss: 0.0993\n",
      "Step 529, Loss: 0.0989\n",
      "Step 539, Loss: 0.0997\n",
      "Step 549, Loss: 0.1007\n",
      "Step 559, Loss: 0.1009\n",
      "Step 569, Loss: 0.1001\n",
      "Step 579, Loss: 0.1003\n",
      "Step 589, Loss: 0.1002\n",
      "Step 599, Loss: 0.0995\n",
      "Step 609, Loss: 0.1000\n",
      "Step 619, Loss: 0.1004\n",
      "Step 629, Loss: 0.1005\n",
      "Step 639, Loss: 0.1003\n",
      "Step 649, Loss: 0.1000\n",
      "Step 659, Loss: 0.1006\n",
      "Step 669, Loss: 0.1009\n",
      "Step 679, Loss: 0.1006\n",
      "Step 689, Loss: 0.1003\n",
      "Step 699, Loss: 0.1003\n",
      "Step 709, Loss: 0.1003\n",
      "Step 719, Loss: 0.1005\n",
      "Step 729, Loss: 0.1009\n",
      "Step 739, Loss: 0.1013\n",
      "Step 749, Loss: 0.1008\n",
      "Step 759, Loss: 0.1008\n",
      "Step 769, Loss: 0.1013\n",
      "Step 779, Loss: 0.1017\n",
      "Step 789, Loss: 0.1014\n",
      "Step 799, Loss: 0.1014\n",
      "Step 809, Loss: 0.1015\n",
      "Step 819, Loss: 0.1025\n",
      "Step 829, Loss: 0.1020\n",
      "Step 839, Loss: 0.1021\n",
      "Step 849, Loss: 0.1018\n",
      "Step 859, Loss: 0.1020\n",
      "Step 869, Loss: 0.1026\n",
      "Step 879, Loss: 0.1031\n",
      "Step 889, Loss: 0.1030\n",
      "Step 899, Loss: 0.1034\n",
      "Step 909, Loss: 0.1035\n",
      "Step 919, Loss: 0.1040\n",
      "Step 929, Loss: 0.1044\n",
      "Step 939, Loss: 0.1047\n",
      "Step 949, Loss: 0.1049\n",
      "Step 959, Loss: 0.1052\n",
      "Step 969, Loss: 0.1060\n",
      "Step 979, Loss: 0.1064\n",
      "Step 989, Loss: 0.1065\n",
      "Step 999, Loss: 0.1062\n",
      "Step 1009, Loss: 0.1063\n",
      "Step 1019, Loss: 0.1061\n",
      "Step 1029, Loss: 0.1067\n",
      "Step 1039, Loss: 0.1077\n",
      "Step 1049, Loss: 0.1082\n",
      "Step 1059, Loss: 0.1086\n",
      "Step 1069, Loss: 0.1085\n",
      "Step 1079, Loss: 0.1085\n",
      "Step 1089, Loss: 0.1084\n",
      "Step 1099, Loss: 0.1081\n",
      "Step 1109, Loss: 0.1084\n",
      "Step 1119, Loss: 0.1082\n",
      "Step 1129, Loss: 0.1087\n",
      "Step 1139, Loss: 0.1087\n",
      "Step 1149, Loss: 0.1087\n",
      "Step 1159, Loss: 0.1090\n",
      "Step 1169, Loss: 0.1093\n",
      "Step 1179, Loss: 0.1096\n",
      "Step 1189, Loss: 0.1096\n",
      "Step 1199, Loss: 0.1098\n",
      "Step 1209, Loss: 0.1098\n",
      "Step 1219, Loss: 0.1101\n",
      "Step 1229, Loss: 0.1107\n",
      "Step 1239, Loss: 0.1110\n",
      "Step 1249, Loss: 0.1110\n",
      "Step 1259, Loss: 0.1113\n",
      "Step 1269, Loss: 0.1116\n",
      "Step 1279, Loss: 0.1114\n",
      "Step 1289, Loss: 0.1116\n",
      "Step 1299, Loss: 0.1119\n",
      "Step 1309, Loss: 0.1118\n",
      "Step 1319, Loss: 0.1121\n",
      "Step 1329, Loss: 0.1123\n",
      "Step 1339, Loss: 0.1123\n",
      "Step 1349, Loss: 0.1124\n",
      "Step 1359, Loss: 0.1128\n",
      "Step 1369, Loss: 0.1130\n",
      "Step 1379, Loss: 0.1130\n",
      "Step 1389, Loss: 0.1132\n",
      "Step 1399, Loss: 0.1132\n",
      "Step 1409, Loss: 0.1134\n",
      "Step 1419, Loss: 0.1139\n",
      "Step 1429, Loss: 0.1137\n",
      "Step 1439, Loss: 0.1137\n",
      "Step 1449, Loss: 0.1140\n",
      "Step 1459, Loss: 0.1142\n",
      "Step 1469, Loss: 0.1147\n",
      "Step 1479, Loss: 0.1152\n",
      "Step 1489, Loss: 0.1154\n",
      "Step 1499, Loss: 0.1153\n",
      "Step 1509, Loss: 0.1154\n",
      "Step 1519, Loss: 0.1152\n",
      "Step 1529, Loss: 0.1154\n",
      "Step 1539, Loss: 0.1158\n",
      "Step 1549, Loss: 0.1161\n",
      "Step 1559, Loss: 0.1163\n",
      "Step 1569, Loss: 0.1166\n",
      "Step 1579, Loss: 0.1171\n",
      "Step 1589, Loss: 0.1173\n",
      "Step 1599, Loss: 0.1175\n",
      "Step 1609, Loss: 0.1175\n",
      "Step 1619, Loss: 0.1175\n",
      "Step 1629, Loss: 0.1176\n",
      "Step 1639, Loss: 0.1176\n",
      "Step 1649, Loss: 0.1174\n",
      "Step 1659, Loss: 0.1178\n",
      "Step 1669, Loss: 0.1179\n",
      "Step 1679, Loss: 0.1181\n",
      "Step 1689, Loss: 0.1180\n",
      "Step 1699, Loss: 0.1180\n",
      "Step 1709, Loss: 0.1182\n",
      "Step 1719, Loss: 0.1186\n",
      "Step 1729, Loss: 0.1184\n",
      "Step 1739, Loss: 0.1185\n",
      "Step 1749, Loss: 0.1185\n",
      "Step 1759, Loss: 0.1191\n",
      "Step 1769, Loss: 0.1193\n",
      "Step 1779, Loss: 0.1197\n",
      "Step 1789, Loss: 0.1197\n",
      "Step 1799, Loss: 0.1202\n",
      "Step 1809, Loss: 0.1202\n",
      "Step 1819, Loss: 0.1205\n",
      "Step 1829, Loss: 0.1206\n",
      "Step 1839, Loss: 0.1209\n",
      "Step 1849, Loss: 0.1208\n",
      "Step 1859, Loss: 0.1208\n",
      "Step 1869, Loss: 0.1211\n",
      "Step 1879, Loss: 0.1209\n",
      "Step 1889, Loss: 0.1208\n",
      "Step 1899, Loss: 0.1206\n",
      "Step 1909, Loss: 0.1209\n",
      "Step 1919, Loss: 0.1212\n",
      "Step 1929, Loss: 0.1214\n",
      "Step 1939, Loss: 0.1215\n",
      "Step 1949, Loss: 0.1217\n",
      "Step 1959, Loss: 0.1221\n",
      "Step 1969, Loss: 0.1223\n",
      "Step 1979, Loss: 0.1224\n",
      "Step 1989, Loss: 0.1226\n",
      "Step 1999, Loss: 0.1227\n",
      "Step 2009, Loss: 0.1227\n",
      "Step 2019, Loss: 0.1229\n",
      "Step 2029, Loss: 0.1230\n",
      "Step 2039, Loss: 0.1231\n",
      "Step 2049, Loss: 0.1231\n",
      "Step 2059, Loss: 0.1232\n",
      "Step 2069, Loss: 0.1231\n",
      "Step 2079, Loss: 0.1233\n",
      "Step 2089, Loss: 0.1236\n",
      "Step 2099, Loss: 0.1237\n",
      "Step 2109, Loss: 0.1239\n",
      "Step 2119, Loss: 0.1240\n",
      "Step 2129, Loss: 0.1240\n",
      "Step 2139, Loss: 0.1238\n",
      "Step 2149, Loss: 0.1236\n",
      "Step 2159, Loss: 0.1236\n",
      "Step 2169, Loss: 0.1237\n",
      "Step 2179, Loss: 0.1236\n",
      "Step 2189, Loss: 0.1236\n",
      "Step 2199, Loss: 0.1236\n",
      "Step 2209, Loss: 0.1237\n",
      "Step 2219, Loss: 0.1237\n",
      "Step 2229, Loss: 0.1239\n",
      "Step 2239, Loss: 0.1240\n",
      "Step 2249, Loss: 0.1239\n",
      "Step 2259, Loss: 0.1238\n",
      "Step 2269, Loss: 0.1240\n",
      "Step 2279, Loss: 0.1240\n",
      "Step 2289, Loss: 0.1240\n",
      "Step 2299, Loss: 0.1240\n",
      "Step 2309, Loss: 0.1238\n",
      "Step 2319, Loss: 0.1238\n",
      "Step 2329, Loss: 0.1238\n",
      "Step 2339, Loss: 0.1240\n",
      "Step 2349, Loss: 0.1241\n",
      "Step 2359, Loss: 0.1241\n",
      "Step 2369, Loss: 0.1241\n",
      "Step 2379, Loss: 0.1240\n",
      "Step 2389, Loss: 0.1240\n",
      "Step 2399, Loss: 0.1240\n",
      "Step 2409, Loss: 0.1239\n",
      "Step 2419, Loss: 0.1240\n",
      "Step 2429, Loss: 0.1239\n",
      "Step 2439, Loss: 0.1238\n",
      "Step 2449, Loss: 0.1236\n",
      "Step 2459, Loss: 0.1237\n",
      "Step 2469, Loss: 0.1237\n",
      "Step 2479, Loss: 0.1237\n",
      "Step 2489, Loss: 0.1240\n",
      "Step 2499, Loss: 0.1240\n",
      "Step 2509, Loss: 0.1240\n",
      "Step 2519, Loss: 0.1240\n",
      "Step 2529, Loss: 0.1239\n",
      "Step 2539, Loss: 0.1239\n",
      "Step 2549, Loss: 0.1239\n",
      "Step 2559, Loss: 0.1240\n",
      "Step 2569, Loss: 0.1238\n",
      "Step 2579, Loss: 0.1238\n",
      "Step 2589, Loss: 0.1236\n",
      "Step 2599, Loss: 0.1236\n",
      "Step 2609, Loss: 0.1236\n",
      "Step 2619, Loss: 0.1234\n",
      "Step 2629, Loss: 0.1233\n",
      "Step 2639, Loss: 0.1233\n",
      "Step 2649, Loss: 0.1233\n",
      "Step 2659, Loss: 0.1234\n",
      "Step 2669, Loss: 0.1234\n",
      "Step 2679, Loss: 0.1233\n",
      "Step 2689, Loss: 0.1234\n",
      "Step 2699, Loss: 0.1234\n",
      "Step 2709, Loss: 0.1233\n",
      "Step 2719, Loss: 0.1234\n",
      "Step 2729, Loss: 0.1235\n",
      "Step 2739, Loss: 0.1233\n",
      "Step 2749, Loss: 0.1234\n",
      "Step 2759, Loss: 0.1234\n",
      "Step 2769, Loss: 0.1236\n",
      "Step 2779, Loss: 0.1236\n",
      "Step 2789, Loss: 0.1236\n",
      "Step 2799, Loss: 0.1237\n",
      "Step 2809, Loss: 0.1239\n",
      "Step 2819, Loss: 0.1240\n",
      "Step 2829, Loss: 0.1241\n",
      "Step 2839, Loss: 0.1241\n",
      "Step 2849, Loss: 0.1242\n",
      "Step 2859, Loss: 0.1243\n",
      "Step 2869, Loss: 0.1244\n",
      "Step 2879, Loss: 0.1248\n",
      "Step 2889, Loss: 0.1249\n",
      "Step 2899, Loss: 0.1249\n",
      "Step 2909, Loss: 0.1250\n",
      "Step 2919, Loss: 0.1251\n",
      "Step 2929, Loss: 0.1250\n",
      "Step 2939, Loss: 0.1250\n",
      "Step 2949, Loss: 0.1251\n",
      "Step 2959, Loss: 0.1250\n",
      "Step 2969, Loss: 0.1251\n",
      "Step 2979, Loss: 0.1251\n",
      "Step 2989, Loss: 0.1250\n",
      "Step 2999, Loss: 0.1251\n",
      "Step 3009, Loss: 0.1253\n",
      "Step 3019, Loss: 0.1253\n",
      "Step 3029, Loss: 0.1253\n",
      "Step 3039, Loss: 0.1253\n",
      "Step 3049, Loss: 0.1254\n",
      "Step 3059, Loss: 0.1255\n",
      "Step 3069, Loss: 0.1254\n",
      "Step 3079, Loss: 0.1255\n",
      "Step 3089, Loss: 0.1256\n",
      "Step 3099, Loss: 0.1256\n",
      "Step 3109, Loss: 0.1256\n",
      "Step 3119, Loss: 0.1255\n",
      "Step 3129, Loss: 0.1255\n",
      "Step 3139, Loss: 0.1254\n",
      "Step 3149, Loss: 0.1255\n",
      "Step 3159, Loss: 0.1256\n",
      "Step 3169, Loss: 0.1256\n",
      "Step 3179, Loss: 0.1256\n",
      "Step 3189, Loss: 0.1256\n",
      "Step 3199, Loss: 0.1256\n",
      "Step 3209, Loss: 0.1256\n",
      "Step 3219, Loss: 0.1255\n",
      "Step 3229, Loss: 0.1255\n",
      "Step 3239, Loss: 0.1254\n",
      "Step 3249, Loss: 0.1254\n",
      "Step 3259, Loss: 0.1254\n",
      "Step 3269, Loss: 0.1254\n",
      "Step 3279, Loss: 0.1256\n",
      "Step 3289, Loss: 0.1255\n",
      "Step 3299, Loss: 0.1256\n",
      "Step 3309, Loss: 0.1255\n",
      "Step 3319, Loss: 0.1255\n",
      "Step 3329, Loss: 0.1256\n",
      "Step 3339, Loss: 0.1256\n",
      "Step 3349, Loss: 0.1257\n",
      "Step 3359, Loss: 0.1257\n",
      "Step 3369, Loss: 0.1256\n",
      "Step 3379, Loss: 0.1256\n",
      "Step 3389, Loss: 0.1255\n",
      "Step 3399, Loss: 0.1255\n",
      "Step 3409, Loss: 0.1256\n",
      "Step 3419, Loss: 0.1257\n",
      "Step 3429, Loss: 0.1256\n",
      "Step 3439, Loss: 0.1259\n",
      "Step 3449, Loss: 0.1258\n",
      "Step 3459, Loss: 0.1259\n",
      "Step 3469, Loss: 0.1260\n",
      "Step 3479, Loss: 0.1259\n",
      "Step 3489, Loss: 0.1259\n",
      "Step 3499, Loss: 0.1259\n",
      "Step 3509, Loss: 0.1259\n",
      "Step 3519, Loss: 0.1258\n",
      "Step 3529, Loss: 0.1259\n",
      "Step 3539, Loss: 0.1259\n",
      "Step 3549, Loss: 0.1260\n",
      "Step 3559, Loss: 0.1260\n",
      "Step 3569, Loss: 0.1260\n",
      "Step 3579, Loss: 0.1261\n",
      "Step 3589, Loss: 0.1261\n",
      "Step 3599, Loss: 0.1262\n",
      "Step 3609, Loss: 0.1261\n",
      "Step 3619, Loss: 0.1262\n",
      "Step 3629, Loss: 0.1261\n",
      "Step 3639, Loss: 0.1262\n",
      "Step 3649, Loss: 0.1261\n",
      "Step 3659, Loss: 0.1262\n",
      "Step 3669, Loss: 0.1262\n",
      "Step 3679, Loss: 0.1261\n",
      "Step 3689, Loss: 0.1261\n",
      "Step 3699, Loss: 0.1261\n",
      "Step 3709, Loss: 0.1262\n",
      "Step 3719, Loss: 0.1262\n",
      "Step 3729, Loss: 0.1262\n",
      "Step 3739, Loss: 0.1263\n",
      "Step 3749, Loss: 0.1263\n",
      "Epoch 1/1, Training Loss: 473.5245, Test Accuracy: 0.9482\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# DataLoader creation\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_datapip, shuffle=True, batch_size=batch_size, collate_fn=preprocess\n",
    ")\n",
    "test_loader = DataLoader(test_datapip, batch_size=batch_size, collate_fn=preprocess)\n",
    "\n",
    "# Define Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=len(list(test_datapip))\n",
    ")\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(model, loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, (inputs, labels) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(**inputs, labels=labels)\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"].to(DEVICE),\n",
    "            attention_mask=inputs[\"attention_mask\"].to(DEVICE),\n",
    "            token_type_ids=inputs[\"token_type_ids\"].to(DEVICE),\n",
    "            labels=labels.to(DEVICE),\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        if step % 10 == 9:\n",
    "            print(f\"Step {step}, Loss: {total_loss / (step + 1):.4f}\")\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for step, (inputs, labels) in enumerate(loader):\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"].to(DEVICE),\n",
    "                attention_mask=inputs[\"attention_mask\"].to(DEVICE),\n",
    "                token_type_ids=inputs[\"token_type_ids\"].to(DEVICE),\n",
    "            )\n",
    "            preds.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    return accuracy_score(true_labels, preds)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, scheduler)\n",
    "    test_accuracy = evaluate(model, test_loader)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_datapip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label: 2, Predicted Label: 2, Text: Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\n",
      "True Label: 3, Predicted Label: 3, Text: The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com) SPACE.com - TORONTO, Canada -- A second\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket.\n",
      "True Label: 3, Predicted Label: 3, Text: Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.\n",
      "True Label: 3, Predicted Label: 3, Text: Prediction Unit Helps Forecast Wildfires (AP) AP - It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.\n",
      "True Label: 3, Predicted Label: 3, Text: Calif. Aims to Limit Farm-Related Smog (AP) AP - Southern California's smog-fighting agency went after emissions of the bovine variety Friday, adopting the nation's first rules to reduce air pollution from dairy cow manure.\n",
      "True Label: 3, Predicted Label: 3, Text: Open Letter Against British Copyright Indoctrination in Schools The British Department for Education and Skills (DfES) recently launched a \"Music Manifesto\" campaign, with the ostensible intention of educating the next generation of British musicians. Unfortunately, they also teamed up with the music industry (EMI, and various artists) to make this popular. EMI has apparently negotiated their end well, so that children in our schools will now be indoctrinated about the illegality of downloading music.The ignorance and audacity of this got to me a little, so I wrote an open letter to the DfES about it. Unfortunately, it's pedantic, as I suppose you have to be when writing to goverment representatives. But I hope you find it useful, and perhaps feel inspired to do something similar, if or when the same thing has happened in your area.\n",
      "True Label: 3, Predicted Label: 3, Text: Loosing the War on Terrorism \\\\\"Sven Jaschan, self-confessed author of the Netsky and Sasser viruses, is\\responsible for 70 percent of virus infections in 2004, according to a six-month\\virus roundup published Wednesday by antivirus company Sophos.\"\\\\\"The 18-year-old Jaschan was taken into custody in Germany in May by police who\\said he had admitted programming both the Netsky and Sasser worms, something\\experts at Microsoft confirmed. (A Microsoft antivirus reward program led to the\\teenager's arrest.) During the five months preceding Jaschan's capture, there\\were at least 25 variants of Netsky and one of the port-scanning network worm\\Sasser.\"\\\\\"Graham Cluley, senior technology consultant at Sophos, said it was staggeri ...\\\\\n",
      "True Label: 3, Predicted Label: 3, Text: FOAFKey: FOAF, PGP, Key Distribution, and Bloom Filters \\\\FOAF/LOAF  and bloom filters have a lot of interesting properties for social\\network and whitelist distribution.\\\\I think we can go one level higher though and include GPG/OpenPGP key\\fingerpring distribution in the FOAF file for simple web-of-trust based key\\distribution.\\\\What if we used FOAF and included the PGP key fingerprint(s) for identities?\\This could mean a lot.  You include the PGP key fingerprints within the FOAF\\file of your direct friends and then include a bloom filter of the PGP key\\fingerprints of your entire whitelist (the source FOAF file would of course need\\to be encrypted ).\\\\Your whitelist would be populated from the social network as your client\\discovered new identit ...\\\\\n",
      "True Label: 3, Predicted Label: 3, Text: E-mail scam targets police chief Wiltshire Police warns about \"phishing\" after its fraud squad chief was targeted.\n",
      "True Label: 3, Predicted Label: 3, Text: Card fraud unit nets 36,000 cards In its first two years, the UK's dedicated card fraud unit, has recovered 36,000 stolen cards and 171 arrests - and estimates it saved 65m.\n"
     ]
    }
   ],
   "source": [
    "# test the first 10 samples in the test dataset\n",
    "for i, (label, text) in enumerate(test_datapip):\n",
    "    if i == 10:\n",
    "        break\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512).to(DEVICE)\n",
    "    outputs = model(**inputs)\n",
    "    pred = torch.argmax(outputs.logits).item()\n",
    "    print(f\"True Label: {label-1}, Predicted Label: {pred}, Text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, data = zip(*list(train_datapip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 30000, 30000, 30000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = list(label)\n",
    "label.count(1), label.count(2), label.count(3), label.count(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLNetTokenizer(name_or_path='xlnet-base-cased', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>', 'additional_special_tokens': ['<eop>', '<eod>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<eod>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"<eop>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
